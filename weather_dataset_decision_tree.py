# -*- coding: utf-8 -*-
"""Untitled62.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lTEfBV7FrSM5uSQAi2Nh-uDXE5ZUN-Dv
"""

# prompt: import weather dataset

import pandas as pd

# Replace 'weather_data.csv' with the actual filename
try:
  weather_data = pd.read_csv('weather_dataset.csv')
  print(weather_data.head()) # Display the first few rows
except FileNotFoundError:
  print("Error: 'weather_data.csv' not found. Please make sure the file exists and the path is correct.")
except pd.errors.ParserError:
  print("Error: Could not parse the CSV file. Please check the file format.")

# prompt: recode the categorical variables to integers

from sklearn.preprocessing import LabelEncoder

# Assuming 'weather_data' DataFrame is already loaded

# Identify categorical columns
categorical_cols = weather_data.select_dtypes(include=['object']).columns

# Create a LabelEncoder object
label_encoder = LabelEncoder()

# Iterate through categorical columns and encode them
for col in categorical_cols:
    weather_data[col] = label_encoder.fit_transform(weather_data[col])

print(weather_data.head())

# prompt: import necessary libraries for building decision tree classifier

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import tree
import matplotlib.pyplot as plt

# prompt: partition the weather_data into train and test

# Assuming 'weather_data' DataFrame is already loaded as in the previous code

# Separate features (X) and target variable (y)
# Replace 'play' with your actual target column name
# Replace the feature columns as needed
X = weather_data.drop(columns=['PlayTennis','Day'])
y = weather_data['PlayTennis']

# prompt: build a decision tree for the X_train with entropy

# Create a decision tree classifier with entropy criterion
clf_BL = DecisionTreeClassifier(criterion="entropy", random_state=42, max_depth=10,ccp_alpha=0.0) # Added max_depth for visualization
clf_BL.fit(X, y)

"""#CHange Parameters
criterion, max depth, ccp alpha
"""

def build_decision_tree(X, y, criterion, max_depth, ccp_alpha):
    # prompt: build a decision tree for the X_train with entropy
    # Create a decision tree classifier with entropy criterion
    clf = DecisionTreeClassifier(criterion=criterion, random_state=42, max_depth=max_depth,ccp_alpha=ccp_alpha) # Added max_depth for visualization
    clf.fit(X, y)
    return clf


clf_BL = build_decision_tree(X, y, criterion="entropy", max_depth=10, ccp_alpha=0.0)
clf_M1 = build_decision_tree(X, y, criterion="gini", max_depth=2, ccp_alpha=0.0)
clf_M2 = build_decision_tree(X, y, criterion="entropy", max_depth=3, ccp_alpha=0.0)
clf_M3 = build_decision_tree(X, y, criterion="entropy", max_depth=10, ccp_alpha=0.05)

clfs = { "BL": clf_BL, "M1": clf_M1, "M2": clf_M2, "M3": clf_M3 }

# Make predictions and evaluate each model
for name, clf in clfs.items():
    y_pred = clf.predict(X)  # Predict on the entire dataset (you might want to use a test set in a real scenario)
    accuracy = accuracy_score(y, y_pred)
    print(f"Model: {name}, Accuracy: {accuracy}")

# prompt: draw a single roc graph for the clf models

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Assuming clfs dictionary and X, y are defined as in the previous code

plt.figure(figsize=(10, 10))

for name, clf in clfs.items():
    y_pred_proba = clf.predict_proba(X)[:, 1]  # Probability estimates for the positive class
    fpr, tpr, _ = roc_curve(y, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line (random classifier)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Decision Tree Models')
plt.legend(loc='lower right')
plt.show()

# prompt: draw a bias variance line graph for the clf_bl decision tree model with train error and test error

# Assuming clfs dictionary and X, y are defined as in the previous code
import numpy as np

plt.figure(figsize=(10, 6))

train_errors = []
test_errors = []
depths = range(1, 11) # Example depths, adjust as needed


for depth in depths:
    clf = DecisionTreeClassifier(max_depth=depth, random_state=42)
    clf.fit(X, y)

    y_pred_train = clf.predict(X)
    train_error = 1 - accuracy_score(y, y_pred_train)
    train_errors.append(train_error)

    #Split data for test error
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #adjust test_size as needed
    clf_test = DecisionTreeClassifier(max_depth=depth, random_state=42) # Retrain on training set
    clf_test.fit(X_train, y_train)

    y_pred_test = clf_test.predict(X_test)
    test_error = 1 - accuracy_score(y_test, y_pred_test)
    test_errors.append(test_error)


plt.plot(depths, train_errors, label='Train Error', marker='o')
plt.plot(depths, test_errors, label='Test Error', marker='x')


plt.xlabel('Tree Depth')
plt.ylabel('Error Rate')
plt.title('Bias-Variance Tradeoff for Decision Tree (clf_BL)') #Rename if you are plotting different model
plt.legend()
plt.grid(True)
plt.show()
